{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14b04c5",
   "metadata": {},
   "source": [
    "# Extended Spotify Analysis \n",
    "**Data Mining Course**\n",
    "\n",
    "This project analyzes personal Spotify streaming history using various data mining techniques:\n",
    "- K-Means Clustering for activity patterns\n",
    "- Apriori Algorithm for artist association rules\n",
    "- Random Forest Regression for future prediction\n",
    "- Statistical analysis and anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf8a18",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "Import all necessary libraries for data processing, analysis, and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Association Rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf001e",
   "metadata": {},
   "source": [
    "## 2. Load Data from JSON Files\n",
    "Load and combine all Spotify streaming history JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all JSON files containing streaming history\n",
    "json_files = [\n",
    "    'Streaming_History_Audio_2023_0.json',\n",
    "    'Streaming_History_Audio_2023_1.json',\n",
    "    'Streaming_History_Audio_2023_2.json',\n",
    "    'Streaming_History_Audio_2023-2024_3.json',\n",
    "    'Streaming_History_Audio_2024_4.json',\n",
    "    'Streaming_History_Audio_2024_5.json',\n",
    "    'Streaming_History_Audio_2024_6.json',\n",
    "    'Streaming_History_Audio_2024_7.json',\n",
    "    'Streaming_History_Audio_2024_8.json',\n",
    "    'Streaming_History_Audio_2024_9.json',\n",
    "    'Streaming_History_Audio_2024-2025_10.json',\n",
    "    'Streaming_History_Audio_2025_11.json',\n",
    "    'Streaming_History_Audio_2025_12.json',\n",
    "    'Streaming_History_Audio_2025_13.json',\n",
    "    'Streaming_History_Audio_2025_14.json'\n",
    "]\n",
    "\n",
    "# Initialize empty list to store all records\n",
    "all_records = []\n",
    "\n",
    "# Read each JSON file and extend the records list\n",
    "for file_name in json_files:\n",
    "    with open(file_name, encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        all_records.extend(data)\n",
    "\n",
    "# Create DataFrame from all records\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "print(f\"Total raw records loaded: {len(df):,}\")\n",
    "print(f\"Columns in dataset: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567204b8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "Clean and prepare the data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5910030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime format. 2 different column names in different files\n",
    "if 'ts' in df.columns:\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "else:\n",
    "    df['ts'] = pd.to_datetime(df['endTime'])\n",
    "\n",
    "# Standardize column names\n",
    "if 'master_metadata_album_artist_name' not in df.columns:\n",
    "    df['master_metadata_album_artist_name'] = df['artistName']\n",
    "if 'master_metadata_track_name' not in df.columns:\n",
    "    df['master_metadata_track_name'] = df['trackName']\n",
    "if 'ms_played' not in df.columns:\n",
    "    df['ms_played'] = df['msPlayed']\n",
    "\n",
    "# Filter: Keep only tracks played for at least 30 seconds, to skip accidental plays\n",
    "df = df[df['ms_played'] >= 30000].copy()\n",
    "\n",
    "# Create new features for analysis\n",
    "df['date'] = df['ts'].dt.date                   \n",
    "df['hour'] = df['ts'].dt.hour                  \n",
    "df['month'] = df['ts'].dt.to_period('M')        \n",
    "df['day_name'] = df['ts'].dt.day_name()         \n",
    "df['weekday'] = df['ts'].dt.dayofweek           \n",
    "df['is_weekend'] = df['weekday'] >= 5            \n",
    "df['hours_played'] = df['ms_played'] / 3600000   \n",
    "\n",
    "print(f\"\\nRecords after filtering (>30s plays): {len(df):,}\")\n",
    "print(f\"Date range: {df['ts'].min().date()} to {df['ts'].max().date()}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86415291",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "Basic statistics and insights about listening habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2498d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics\n",
    "total_listening_hours = df['hours_played'].sum()\n",
    "unique_artists_count = df['master_metadata_album_artist_name'].nunique()\n",
    "unique_tracks_count = df['master_metadata_track_name'].nunique()\n",
    "most_played_artist = df['master_metadata_album_artist_name'].mode()[0]\n",
    "most_played_track = df['master_metadata_track_name'].mode()[0]\n",
    "most_played_track_count = df['master_metadata_track_name'].value_counts().iloc[0] # value counts: how many times repeated results as a list. \n",
    "# different from mode which gives only the most frequent value.\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"LISTENING STATISTICS SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Listening Time: {total_listening_hours:.2f} hours\") #.2f only last two difits \n",
    "print(f\"Unique Artists: {unique_artists_count}\")\n",
    "print(f\"Unique Tracks: {unique_tracks_count}\")\n",
    "print(f\"Most Played Artist: {most_played_artist}\")\n",
    "print(f\"Most Played Track: {most_played_track} ({most_played_track_count} plays)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802a102",
   "metadata": {},
   "source": [
    "### 4.1 Weekly Activity Heatmap\n",
    "Visualize listening patterns by day of week and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7413e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define day order for proper sorting\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Group by day and hour to count plays\n",
    "heatmap_data = df.groupby(['day_name', 'hour']).size().reset_index(name='play_count') #resets index after groupby, and makes it 'play_count'\n",
    "# output will be a dataframe with three columns: day_name, hour, play_count indexed by play_count\n",
    "\n",
    "# Sort days properly\n",
    "heatmap_data['day_name'] = pd.Categorical(heatmap_data['day_name'], categories=days_order, ordered=True)\n",
    "\n",
    "# Pivot to create matrix for heatmap\n",
    "heatmap_matrix = heatmap_data.pivot(index='day_name', columns='hour', values='play_count').fillna(0)\n",
    "\n",
    "# Create heatmap visualization\n",
    "fig_heatmap = px.imshow(heatmap_matrix,\n",
    "                        labels=dict(x=\"Hour of Day\", y=\"Day of Week\", color=\"Plays\"),\n",
    "                        title='Weekly Listening Pattern Heatmap',\n",
    "                        color_continuous_scale='Greens')\n",
    "display(fig_heatmap)\n",
    "\n",
    "print(\"Heatmap shows when you listen to music the most during the week.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974aaae1",
   "metadata": {},
   "source": [
    "### 4.2 Top 10 Most Played Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 tracks\n",
    "top_10_tracks = df['master_metadata_track_name'].value_counts().head(10).reset_index()\n",
    "top_10_tracks.columns = ['Track Name', 'Play Count']\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig_top10 = px.bar(top_10_tracks, \n",
    "                   x='Play Count', \n",
    "                   y='Track Name', \n",
    "                   orientation='h',\n",
    "                   title='Top 10 Most Played Tracks of All Time',\n",
    "                   color='Play Count', \n",
    "                   color_continuous_scale='Greens')\n",
    "\n",
    "# Reverse y-axis to show highest at top\n",
    "fig_top10.update_layout(yaxis=dict(autorange=\"reversed\"))\n",
    "fig_top10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9064a9",
   "metadata": {},
   "source": [
    "### 4.3 Trend Analysis: Forgotten vs Rising Artists\n",
    "Identify artists you used to listen to but stopped, and new favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time periods: in my case it is last 90 days vs before that\n",
    "last_date = df['ts'].max()\n",
    "cutoff_date = last_date - pd.Timedelta(days=90) #TimeDelta is used to subtract days from a date.\n",
    "\n",
    "# Split data into past and recent\n",
    "past_data = df[df['ts'] < cutoff_date]\n",
    "recent_data = df[df['ts'] >= cutoff_date]\n",
    "\n",
    "# Calculate average plays per month for each artist in both periods\n",
    "past_months = len(past_data['month'].unique()) or 1 # or 1 is used to avoid division by zero errors.\n",
    "recent_months = len(recent_data['month'].unique()) or 1\n",
    "\n",
    "past_artist_stats = past_data.groupby('master_metadata_album_artist_name').size() / past_months\n",
    "recent_artist_stats = recent_data.groupby('master_metadata_album_artist_name').size() / recent_months\n",
    "\n",
    "# Forgotten Artists: played a lot before (>5/month) but rarely now (<1/month)\n",
    "forgotten_artists = past_artist_stats[\n",
    "    (past_artist_stats > 5) & \n",
    "    (recent_artist_stats.reindex(past_artist_stats.index).fillna(0) < 1)\n",
    "].sort_values(ascending=False).head(5)\n",
    "\n",
    "# Rising Artists: playing a lot now (>10/month) and at least 2x more than before\n",
    "rising_artists = recent_artist_stats[\n",
    "    (recent_artist_stats > 10) & \n",
    "    (recent_artist_stats > past_artist_stats.reindex(recent_artist_stats.index).fillna(0) * 2)\n",
    "].sort_values(ascending=False).head(5)\n",
    "\n",
    "print(\"\\nðŸ“‰ FORGOTTEN ARTISTS (used to listen, now don't):\")\n",
    "for artist, plays_per_month in forgotten_artists.items():\n",
    "    print(f\"  â€¢ {artist}: {plays_per_month:.1f} plays/month in the past\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ RISING ARTISTS (new favorites):\")\n",
    "for artist, plays_per_month in rising_artists.items():\n",
    "    print(f\"  â€¢ {artist}: {plays_per_month:.1f} plays/month recently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ead97",
   "metadata": {},
   "source": [
    "## 5. K-Means Clustering Analysis\n",
    "Group hours of the day by listening activity patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b6739",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Data for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c877e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by hour of day\n",
    "hourly_stats = df.groupby('hour').agg({\n",
    "    'ms_played': 'sum',                        # Total time played\n",
    "    'master_metadata_track_name': 'count'      # Number of plays\n",
    "}).rename(columns={'master_metadata_track_name': 'play_count'})\n",
    "\n",
    "# Convert to hours for better interpretation\n",
    "hourly_stats['hours_played'] = hourly_stats['ms_played'] / 3600000\n",
    "\n",
    "# Prepare features for clustering: [hours_played, play_count]\n",
    "X_clustering = hourly_stats[['hours_played', 'play_count']].values\n",
    "\n",
    "# Standardize features (important for K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clustering)\n",
    "\n",
    "print(\"Data prepared for clustering:\")\n",
    "print(hourly_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2e907",
   "metadata": {},
   "source": [
    "### 5.2 Elbow Method to Find Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241872fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WCSS (Within-Cluster Sum of Squares) for k=1 to k=10\n",
    "wcss_values = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_scaled)\n",
    "    wcss_values.append(kmeans_temp.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "fig_elbow = px.line(x=list(k_range), y=wcss_values, markers=True,\n",
    "                    title='Elbow Method for Optimal K',\n",
    "                    labels={'x': 'Number of Clusters (k)', 'y': 'WCSS (Inertia)'})\n",
    "fig_elbow.add_annotation(x=3, y=wcss_values[2],\n",
    "                         text=\"Elbow Point (k=3)\",\n",
    "                         showarrow=True, arrowhead=2)\n",
    "fig_elbow.show()\n",
    "\n",
    "print(\"The 'elbow' in the graph suggests the optimal number of clusters.\")\n",
    "print(\"Based on the curve, k=3 appears to be a good choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861496a0",
   "metadata": {},
   "source": [
    "### 5.3 Apply K-Means with k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101212f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering with k=3\n",
    "optimal_k = 3\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "hourly_stats['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Assign meaningful names to clusters based on play_count\n",
    "cluster_means = hourly_stats.groupby('cluster')['play_count'].mean().sort_values()\n",
    "cluster_labels = {\n",
    "    cluster_means.index[0]: 'Low Activity',\n",
    "    cluster_means.index[1]: 'Medium Activity',\n",
    "    cluster_means.index[2]: 'High Activity'\n",
    "}\n",
    "hourly_stats['cluster_label'] = hourly_stats['cluster'].map(cluster_labels)\n",
    "\n",
    "# Calculate clustering quality metrics\n",
    "silhouette = silhouette_score(X_scaled, hourly_stats['cluster'])\n",
    "davies_bouldin = davies_bouldin_score(X_scaled, hourly_stats['cluster'])\n",
    "\n",
    "print(f\"\\nClustering Quality Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette:.3f} (higher is better, range: -1 to 1)\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "\n",
    "# Show which hours belong to which cluster\n",
    "print(\"\\nHours grouped by activity level:\")\n",
    "for label in ['Low Activity', 'Medium Activity', 'High Activity']:\n",
    "    hours = hourly_stats[hourly_stats['cluster_label'] == label].index.tolist()\n",
    "    print(f\"{label}: Hours {hours}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8af953",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot with clusters\n",
    "fig_kmeans = px.scatter(hourly_stats.reset_index(),\n",
    "                        x='hour',\n",
    "                        y='play_count',\n",
    "                        color='cluster_label',\n",
    "                        size='hours_played',\n",
    "                        title='Hourly Activity Clusters (K-Means)',\n",
    "                        labels={'hour': 'Hour of Day', 'play_count': 'Number of Plays'},\n",
    "                        hover_data=['hours_played'])\n",
    "fig_kmeans.show()\n",
    "\n",
    "print(\"Each point represents an hour. Colors show activity clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cb931",
   "metadata": {},
   "source": [
    "## 6. Association Rules Mining (Apriori Algorithm)\n",
    "Discover which artists are frequently listened to together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a56279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# Remove rows with missing artist names\n",
    "df_clean = df.dropna(subset=['master_metadata_album_artist_name'])\n",
    "\n",
    "# Filter: Keep only artists listened to on at least 10 different days\n",
    "# This improves performance and focuses on regular artists\n",
    "artist_frequency = df_clean.groupby('master_metadata_album_artist_name')['date'].nunique()\n",
    "frequent_artists = artist_frequency[artist_frequency >= 10].index\n",
    "df_filtered = df_clean[df_clean['master_metadata_album_artist_name'].isin(frequent_artists)]\n",
    "\n",
    "# Create transactions: group artists by date\n",
    "# Each date is a transaction containing all artists listened that day\n",
    "transactions = df_filtered.groupby('date')['master_metadata_album_artist_name'].apply(\n",
    "    lambda x: list(set([str(artist) for artist in x if str(artist) != 'nan']))\n",
    ").tolist()\n",
    "\n",
    "print(f\"Total transactions (days): {len(transactions)}\")\n",
    "print(f\"Artists considered: {len(frequent_artists)}\")\n",
    "\n",
    "# Initialize variables for results\n",
    "association_rules_df = pd.DataFrame()\n",
    "fig_apriori = None\n",
    "top_artist_pairs = pd.DataFrame()\n",
    "\n",
    "if len(transactions) > 0:\n",
    "    # Transform transactions into binary matrix format\n",
    "    te = TransactionEncoder()\n",
    "    te_array = te.fit_transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    # Apply Apriori algorithm\n",
    "    # min_support=0.02 means artist pair must appear in at least 2% of days\n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.02, use_colnames=True)\n",
    "    \n",
    "    if not frequent_itemsets.empty:\n",
    "        print(f\"Frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "        \n",
    "        # Generate association rules\n",
    "        rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "        association_rules_df = rules.sort_values('lift', ascending=False)\n",
    "        \n",
    "        if not association_rules_df.empty:\n",
    "            # Convert frozenset to string for display\n",
    "            association_rules_df['antecedents_str'] = association_rules_df['antecedents'].apply(\n",
    "                lambda x: ', '.join(list(x)))\n",
    "            association_rules_df['consequents_str'] = association_rules_df['consequents'].apply(\n",
    "                lambda x: ', '.join(list(x)))\n",
    "            \n",
    "            print(f\"Association rules generated: {len(association_rules_df)}\\n\")\n",
    "            \n",
    "            # Visualization: Support vs Confidence, sized by Lift\n",
    "            fig_apriori = px.scatter(association_rules_df,\n",
    "                                     x='support',\n",
    "                                     y='confidence',\n",
    "                                     size='lift',\n",
    "                                     color='lift',\n",
    "                                     hover_data=['antecedents_str', 'consequents_str'],\n",
    "                                     title='Artist Association Rules (Apriori)',\n",
    "                                     labels={'support': 'Support', \n",
    "                                            'confidence': 'Confidence', \n",
    "                                            'lift': 'Lift'},\n",
    "                                     color_continuous_scale='Viridis')\n",
    "            fig_apriori.show()\n",
    "            \n",
    "            # Create top pairs table\n",
    "            total_days = len(transactions)\n",
    "            association_rules_df['co_listen_count'] = (association_rules_df['support'] * total_days).astype(int)\n",
    "            \n",
    "            top_artist_pairs = association_rules_df[\n",
    "                ['antecedents_str', 'consequents_str', 'co_listen_count']\n",
    "            ].head(10)\n",
    "            top_artist_pairs.columns = ['Artist 1', 'Artist 2', 'Days Listened Together']\n",
    "            \n",
    "            print(\"\\nTop 10 Artist Pairs Listened Together:\")\n",
    "            print(top_artist_pairs.to_string(index=False))\n",
    "            \n",
    "            # Explain metrics\n",
    "            print(\"\\nðŸ“Š Metrics Explanation:\")\n",
    "            print(\"  â€¢ Support: How often the artist pair appears together\")\n",
    "            print(\"  â€¢ Confidence: Probability of listening to Artist 2 when listening to Artist 1\")\n",
    "            print(\"  â€¢ Lift: How much more likely to listen to both together vs separately\")\n",
    "            print(\"    (Lift > 1 means positive association)\")\n",
    "        else:\n",
    "            print(\"No association rules found.\")\n",
    "    else:\n",
    "        print(\"No frequent itemsets found. Try lowering min_support.\")\n",
    "else:\n",
    "    print(\"No transaction data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba8974",
   "metadata": {},
   "source": [
    "## 7. Statistical Tests and Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2ec5f",
   "metadata": {},
   "source": [
    "### 7.1 T-Test: Weekday vs Weekend Listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce79c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate weekday and weekend listening hours\n",
    "weekday_listening = df[~df['is_weekend']]['hours_played']\n",
    "weekend_listening = df[df['is_weekend']]['hours_played']\n",
    "\n",
    "# Perform independent t-test\n",
    "t_statistic, p_value = scipy_stats.ttest_ind(weekday_listening, weekend_listening, nan_policy='omit')\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"T-TEST: Weekday vs Weekend Listening Habits\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Weekday Average: {weekday_listening.mean():.4f} hours per play\")\n",
    "print(f\"Weekend Average: {weekend_listening.mean():.4f} hours per play\")\n",
    "print(f\"\\nT-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"\\nConclusion: \", end=\"\")\n",
    "if p_value < 0.05:\n",
    "    print(\"There IS a statistically significant difference (p < 0.05)\")\n",
    "else:\n",
    "    print(\"There is NO statistically significant difference (p >= 0.05)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d14f2d",
   "metadata": {},
   "source": [
    "### 7.2 Anomaly Detection Using Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plays per day\n",
    "daily_play_counts = df.groupby('date').size()\n",
    "\n",
    "# Calculate Z-scores (how many standard deviations from mean)\n",
    "mean_plays = daily_play_counts.mean()\n",
    "std_plays = daily_play_counts.std()\n",
    "z_scores = (daily_play_counts - mean_plays) / std_plays\n",
    "\n",
    "# Identify anomalies: Z-score > 3 or < -3 (outliers)\n",
    "anomaly_threshold = 3\n",
    "anomaly_days = daily_play_counts[z_scores.abs() > anomaly_threshold]\n",
    "\n",
    "print(f\"\\nAnomaly Detection Results:\")\n",
    "print(f\"Total days analyzed: {len(daily_play_counts)}\")\n",
    "print(f\"Anomalous days detected: {len(anomaly_days)}\")\n",
    "print(f\"Average daily plays: {mean_plays:.1f}\")\n",
    "\n",
    "if not anomaly_days.empty:\n",
    "    print(f\"\\nMost extreme anomaly:\")\n",
    "    max_anomaly_date = anomaly_days.idxmax()\n",
    "    print(f\"  Date: {max_anomaly_date}\")\n",
    "    print(f\"  Plays: {anomaly_days.max()} (Z-score: {z_scores[max_anomaly_date]:.2f})\")\n",
    "\n",
    "# Visualize daily plays with anomalies highlighted\n",
    "fig_anomaly = go.Figure()\n",
    "\n",
    "# Normal days\n",
    "fig_anomaly.add_trace(go.Scatter(\n",
    "    x=daily_play_counts.index,\n",
    "    y=daily_play_counts.values,\n",
    "    mode='lines',\n",
    "    name='Daily Plays',\n",
    "    line=dict(color='lightblue')\n",
    "))\n",
    "\n",
    "# Anomaly days\n",
    "if not anomaly_days.empty:\n",
    "    fig_anomaly.add_trace(go.Scatter(\n",
    "        x=anomaly_days.index,\n",
    "        y=anomaly_days.values,\n",
    "        mode='markers',\n",
    "        name='Anomalies',\n",
    "        marker=dict(color='red', size=10, symbol='x')\n",
    "    ))\n",
    "\n",
    "fig_anomaly.update_layout(\n",
    "    title='Daily Listening with Anomaly Detection',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Number of Plays'\n",
    ")\n",
    "fig_anomaly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb18900",
   "metadata": {},
   "source": [
    "## 8. Time Series Forecasting with Random Forest\n",
    "Predict future monthly listening trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef7f97",
   "metadata": {},
   "source": [
    "### 8.1 Prepare Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate plays by month\n",
    "monthly_play_counts = df.groupby('month').size()\n",
    "\n",
    "# Create time index (0, 1, 2, 3, ... for each month)\n",
    "X_time = np.arange(len(monthly_play_counts)).reshape(-1, 1)\n",
    "y_plays = monthly_play_counts.values\n",
    "\n",
    "# Add seasonality features using sine and cosine\n",
    "# This helps model capture monthly patterns (e.g., summer vs winter)\n",
    "month_numbers = np.array([period.month for period in monthly_play_counts.index]).reshape(-1, 1)\n",
    "\n",
    "# Combine features: [time_index, sin(month), cos(month)]\n",
    "X_features = np.hstack([\n",
    "    X_time,\n",
    "    np.sin(2 * np.pi * month_numbers / 12),  # Seasonal sine\n",
    "    np.cos(2 * np.pi * month_numbers / 12)   # Seasonal cosine\n",
    "])\n",
    "\n",
    "print(f\"Training data: {len(monthly_play_counts)} months\")\n",
    "print(f\"Features per month: {X_features.shape[1]} (time + seasonality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470080f4",
   "metadata": {},
   "source": [
    "### 8.2 Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e58d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    random_state=42,       # For reproducibility\n",
    "    n_jobs=-1              # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_features, y_plays)\n",
    "\n",
    "# Evaluate model on training data\n",
    "y_predicted = rf_model.predict(X_features)\n",
    "r2 = r2_score(y_plays, y_predicted)\n",
    "rmse = np.sqrt(mean_squared_error(y_plays, y_predicted))\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"RÂ² Score: {r2:.3f} (1.0 is perfect fit)\")\n",
    "print(f\"RMSE: {rmse:.2f} plays\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = ['Time Trend', 'Seasonality (sin)', 'Seasonality (cos)']\n",
    "importances = rf_model.feature_importances_\n",
    "print(f\"\\nFeature Importance:\")\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    print(f\"  {name}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e999256",
   "metadata": {},
   "source": [
    "### 8.3 Generate Future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict next 6 months\n",
    "forecast_months = 6\n",
    "last_time_index = X_time[-1][0]\n",
    "last_date = monthly_play_counts.index[-1].to_timestamp()\n",
    "\n",
    "# Create future time indices\n",
    "future_time_indices = np.array([[last_time_index + i] for i in range(1, forecast_months + 1)])\n",
    "\n",
    "# Calculate future month numbers (wraps around: 12 -> 1)\n",
    "last_month_num = month_numbers[-1][0]\n",
    "future_month_numbers = np.array([[(last_month_num + i - 1) % 12 + 1] \n",
    "                                 for i in range(1, forecast_months + 1)])\n",
    "\n",
    "# Create future feature matrix with seasonality\n",
    "X_future = np.hstack([\n",
    "    future_time_indices,\n",
    "    np.sin(2 * np.pi * future_month_numbers / 12),\n",
    "    np.cos(2 * np.pi * future_month_numbers / 12)\n",
    "])\n",
    "\n",
    "# Make predictions\n",
    "future_predictions = rf_model.predict(X_future)\n",
    "\n",
    "# Generate future month labels\n",
    "future_month_labels = []\n",
    "for i in range(1, forecast_months + 1):\n",
    "    future_date = last_date + pd.DateOffset(months=i)\n",
    "    future_month_labels.append(future_date.strftime('%Y-%m'))\n",
    "\n",
    "# Display predictions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FORECAST: Next 6 Months\")\n",
    "print(\"=\"*50)\n",
    "for month, prediction in zip(future_month_labels, future_predictions):\n",
    "    print(f\"{month}: {int(prediction)} predicted plays\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe47058",
   "metadata": {},
   "source": [
    "### 8.4 Visualize Historical Data and Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb69033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "historical_months = [str(period) for period in monthly_play_counts.index]\n",
    "\n",
    "# Create figure\n",
    "fig_forecast = go.Figure()\n",
    "\n",
    "# 1. Actual historical data (bars)\n",
    "fig_forecast.add_trace(go.Bar(\n",
    "    x=historical_months,\n",
    "    y=y_plays,\n",
    "    name='Actual Plays',\n",
    "    marker_color='#1DB954'  # Spotify green\n",
    "))\n",
    "\n",
    "# 2. Model fit on historical data (orange line)\n",
    "fig_forecast.add_trace(go.Scatter(\n",
    "    x=historical_months,\n",
    "    y=y_predicted,\n",
    "    name='Model Fit',\n",
    "    line=dict(color='orange', width=2),\n",
    "    mode='lines'\n",
    "))\n",
    "\n",
    "# 3. Future predictions (red dashed line)\n",
    "# Connect last historical point to forecast for smooth transition\n",
    "forecast_x = [historical_months[-1]] + future_month_labels\n",
    "forecast_y = [y_predicted[-1]] + list(future_predictions)\n",
    "\n",
    "fig_forecast.add_trace(go.Scatter(\n",
    "    x=forecast_x,\n",
    "    y=forecast_y,\n",
    "    name='Forecast',\n",
    "    line=dict(color='red', width=3, dash='dot'),\n",
    "    mode='lines+markers'\n",
    "))\n",
    "\n",
    "# Layout\n",
    "fig_forecast.update_layout(\n",
    "    title='Monthly Listening History and 6-Month Forecast',\n",
    "    xaxis_title='Month',\n",
    "    yaxis_title='Number of Plays',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig_forecast.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verimadenciligi (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
